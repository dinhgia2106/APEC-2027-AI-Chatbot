"""Build a FAISS index from embeddings generated by backend/Embed/embed_data.py.

Usage:
    pip install faiss-cpu numpy
    python backend/VectorStore/build_faiss_index.py

Outputs (stored in backend/VectorStore):
    - apec.index   : serialized FAISS index
    - id2key.json  : list mapping vector id -> key path
"""

import json
from pathlib import Path

import faiss  # type: ignore
import numpy as np

DIM = 768  # Embedding dimension for paraphrase-multilingual-mpnet-base-v2


def load_data(embed_dir: Path):
    """Load embeddings.npy and keys.txt from the Embed directory."""
    embeddings = np.load(embed_dir / "embeddings.npy")
    keys = (embed_dir / "keys.txt").read_text(encoding="utf-8").splitlines()

    if embeddings.shape[0] != len(keys):
        raise ValueError("Embeddings and keys size mismatch.")

    embeddings = embeddings.astype("float32", copy=False)
    return embeddings, keys


def build_index(vectors: np.ndarray):
    index = faiss.IndexFlatIP(DIM)  # inner product ~ cosine because vectors are normalized
    index.add(vectors)
    return index


def main():
    script_dir = Path(__file__).resolve().parent
    embed_dir = script_dir.parent / "Embed"  # backend/Embed

    if not (embed_dir / "embeddings.npy").exists():
        raise FileNotFoundError("embeddings.npy not found. Run embed_data.py first.")

    embeddings, keys = load_data(embed_dir)

    index = build_index(embeddings)

    # Persist index & mapping in VectorStore directory (script_dir)
    faiss.write_index(index, str(script_dir / "apec.index"))
    with (script_dir / "id2key.json").open("w", encoding="utf-8") as f:
        json.dump(keys, f, ensure_ascii=False, indent=2)

    print(f"FAISS index saved to {script_dir} with {len(keys)} vectors.")


if __name__ == "__main__":
    main() 